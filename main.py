import streamlit as st
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
import google.generativeai as genai
from langchain.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Load custom CSS from a file (not a Streamlit command)
# with open("style.css") as f:
#     custom_css = f.read()

# Get the Google API key from the environment variables
google_api_key = os.getenv("GOOGLE_API_KEY")
if google_api_key is None:
    raise ValueError("GOOGLE_API_KEY environment variable is not set")

# Configure the Google Generative AI API key
genai.configure(api_key=google_api_key)


def get_pdf_text(pdf_docs):
    text = ""
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text


def get_text_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)
    chunks = text_splitter.split_text(text)
    return chunks


def get_vector_store(text_chunks):
    embeddings = GoogleGenerativeAIEmbeddings(api_key=google_api_key, model="models/embedding-001")
    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)
    vector_store.save_local("faiss_index")


def get_conversational_chain():
    prompt_template = """
    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in
    provided context just say, "answer is not available in the context", don't provide the wrong answer\n\n
    Context:\n {context}?\n
    Question: \n{question}\n

    Answer:
    """
    model = ChatGoogleGenerativeAI(api_key=google_api_key, model="gemini-pro", temperature=0.3)
    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    chain = load_qa_chain(model, chain_type="stuff", prompt=prompt)

    return chain


def main():
    # Set the page configuration as the first Streamlit command
    st.set_page_config(page_title="Chat with PDF")
    st.header("Chat with AI")

    # Assignment Details
    st.subheader("Assignment Details")
    st.text("""
        Create a small AI chat application that allows users to upload PDFs to serve as a knowledge base. 
        The AI chat interface should enable users to ask questions related to the uploaded files and receive accurate responses based on the content.
        """)

    st.subheader("Requirements:")
    st.markdown("""
        1. **AI Chat Interface:**
           - Build a chat interface where users can interact with the AI to ask questions about the PDFs in the knowledge base.

        2. **LLM Integration:**
           - Utilize open-source Large Language Models (LLMs) by creating a free account on platforms such as Hugging Face or Gro or Gemini free APIs.

        3. **Retrieval-Augmented Generation (RAG):**
           - Implement Retrieval-Augmented Generation (RAG) to enhance the quality of responses by combining document retrieval and AI-generated content.

        4. **API Development:**
           - Provide at least one API endpoint to handle user queries and return responses generated by the LLM.

        5. **Unit Testing:**
           - Write unit tests to cover all functionalities of the application to ensure robustness and correctness.

        6. **User Interface (UI):**
           - Use Streamlit to build a simple, user-friendly interface for the chat application.
        """)
    user_question = st.text_input("Ask a Question from the PDF Files")

    if user_question:
        user_input(user_question)

    with st.sidebar:
        st.title("Menu:")
        pdf_docs = st.file_uploader("Upload your PDF Files and Click on the Submit & Process Button",
                                    accept_multiple_files=True)
        if st.button("Submit & Process"):
            with st.spinner("Processing..."):
                raw_text = get_pdf_text(pdf_docs)
                text_chunks = get_text_chunks(raw_text)
                get_vector_store(text_chunks)
                st.success("Done")


def user_input(user_question):
    embeddings = GoogleGenerativeAIEmbeddings(api_key=google_api_key, model="models/embedding-001")
    new_db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
    docs = new_db.similarity_search(user_question)

    chain = get_conversational_chain()
    response = chain({"input_documents": docs, "question": user_question}, return_only_outputs=True)
    st.write("Reply: ", response["output_text"])


if __name__ == "__main__":
    main()
